{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "import qdrant_client\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import timeit\n",
    "import time\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "#from llama_index.core import  get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor,PrevNextNodePostprocessor,KeywordNodePostprocessor\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "#from llama_index.core.extractors import (\n",
    "#    TitleExtractor,\n",
    "#    QuestionsAnsweredExtractor,\n",
    "#)\n",
    "#from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.core.indices.postprocessor import SentenceTransformerRerank\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "from llama_index.core.vector_stores.types import ExactMatchFilter, MetadataFilters, MetadataFilter\n",
    "from llama_index.core import PromptTemplate\n",
    "### chromaDB\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "index_name=\"Pardal\"\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    \"http://localhost:6333\",\n",
    "    #api_key=\"<qdrant-api-key>\", # For Qdrant Cloud, None for local instance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_qa_template_str = (\n",
    "    \"Context information is\"\n",
    "    \" below.\\n---------------------\\n{context_str}\\n---------------------\\nUsing\"\n",
    "    \" the context information and not prior knowledge, answer\"\n",
    "    \" the question: {query_str}\\nIf the context isn't helpful, you can also\"\n",
    "    \" answer the question on your own.\"\n",
    "    \"Respond always in the language portuguese from portugal and do not use any word that is specific from brazil.\"\n",
    "    \"Cite the source of the new information if you use it.\\n\"\n",
    "    \"Answer:\\n\"\n",
    ")\n",
    "text_qa_template = PromptTemplate(text_qa_template_str)\n",
    "\n",
    "refine_template_str = (\n",
    "    \"The original question is as follows: {query_str}\\nWe have provided an\"\n",
    "    \" existing answer: {existing_answer}\\nWe have the opportunity to refine\"\n",
    "    \" the existing answer (only if needed) with some more context\"\n",
    "    \" below.\\n------------\\n{context_msg}\\n------------\\nUsing the\"\n",
    "    \" context information and not prior knowledge, update or repeat the existing answer.\\n\"\n",
    "    \" Respond always in the language portuguese from portugal and do not use any word that is specific from brazil.\\n\"\n",
    "    \"Cite the source of the new information if you use it.\\n\"\n",
    "    \"If the context isn't useful, return the original answer.\\n\"\n",
    "    \"Refined Answer: \"\n",
    ")\n",
    "refine_template = PromptTemplate(refine_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index...\n",
      "Constructing query engine...\n",
      "filtro should=[FieldCondition(key='Substancia', match=MatchAny(any=['Adalimumab']), range=None, geo_bounding_box=None, geo_radius=None, geo_polygon=None, values_count=None)] min_should=None must=None must_not=None\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='Empty Response', source_nodes=[], metadata=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM_URL=\"https://gecadllm.fish-albacore.ts.net:8443/api\"\n",
    "embed_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "def retrieve_index(client, llm, index_name):\n",
    "    text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "    Settings.text_splitter = text_splitter\n",
    "\n",
    "    # embed_model = OpenAIEmbedding(embed_batch_size=10)\n",
    "    embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=embed_model_name))\n",
    "    Settings.llm = llm\n",
    "    Settings.embed_model = embed_model\n",
    "    # Settings.num_output = 512\n",
    "    # Settings.context_window = 3900\n",
    "    Settings.chunk_size = 1024\n",
    "    Settings.chunk_overlap = 64\n",
    "\n",
    "    vector_store = QdrantVectorStore(client=client, collection_name=index_name)\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store,\n",
    "        text_qa_template=text_qa_template,\n",
    "        refine_template=refine_template,\n",
    "        transformations=[text_splitter],\n",
    "    )\n",
    "\n",
    "    return index\n",
    "\n",
    "from qdrant_client.http.models import Filter, FieldCondition, MatchValue, MatchAny\n",
    "\n",
    "\n",
    "def get_filters_qdrant(metadatasource, products):\n",
    "    filters = []\n",
    "    f = create_filters(products=products, metadatasource=metadatasource)\n",
    "\n",
    "    if len(f[\"Nome_Comercial\"]) > 0:\n",
    "        filters.append(\n",
    "            FieldCondition(\n",
    "                key=\"Nome_Comercial\",\n",
    "                match=MatchAny(any=list(set(f[\"Nome_Comercial\"]))),\n",
    "            )\n",
    "        )\n",
    "    if len(f[\"Substancia\"]) > 0:\n",
    "        filters.append(\n",
    "            FieldCondition(\n",
    "                key=\"Substancia\",\n",
    "                match=MatchAny(any=list(set(f[\"Substancia\"]))),\n",
    "            )\n",
    "        )\n",
    "    # return f\n",
    "    if len(filters) == 0:\n",
    "        return Filter(should=[])\n",
    "    if len(filters) > 0:\n",
    "        return Filter(should=filters)\n",
    "\n",
    "\n",
    "def create_filters(products, metadatasource):\n",
    "    f = {\"Nome_Comercial\": [], \"Substancia\": []}\n",
    "    for word in products.split(\",\"):\n",
    "        # print(word)\n",
    "        word = word.strip()\n",
    "        if any(\n",
    "            word.lower() == item.lower() for item in metadatasource[\"Nome Comercial\"]\n",
    "        ):\n",
    "            # print(\"found\")\n",
    "            val = metadatasource[\n",
    "                [\n",
    "                    word.lower() == item.lower()\n",
    "                    for item in metadatasource[\"Nome Comercial\"]\n",
    "                ]\n",
    "            ][\"Nome Comercial\"].values\n",
    "            print(word, \"comer\")\n",
    "            for v in val:\n",
    "                f[\"Nome_Comercial\"].append(v)\n",
    "        #  f.append(MetadataFilter(key=\"Nome_Comercial\", value=val, operator=\"==\"))\n",
    "        elif any(word.lower() == str(item).lower() for item in metadatasource[\"Subs\"]):\n",
    "            val = metadatasource[\n",
    "                [word.lower() == str(item).lower() for item in metadatasource[\"Subs\"]]\n",
    "            ][\"Subs\"].values\n",
    "\n",
    "            for v in val:\n",
    "                # f[v] = \"Substancia\"\n",
    "                f[\"Substancia\"].append(v)\n",
    "\n",
    "        if metadatasource[\"Nome Comercial\"].str.contains(word, case=False).any():\n",
    "            # print(\"found\")\n",
    "            val = metadatasource[\n",
    "                metadatasource[\"Nome Comercial\"].str.contains(word, case=False)\n",
    "            ][\"Nome Comercial\"].values\n",
    "            print(word, \"comer\")\n",
    "            for v in val:\n",
    "                # f[v] = \"Nome_Comercial\"\n",
    "                f[\"Nome_Comercial\"].append(v)\n",
    "\n",
    "        #  f.append(MetadataFilter(key=\"Nome_Comercial\", value=val, operator=\"==\"))\n",
    "        elif (\n",
    "            metadatasource[metadatasource[\"Subs\"].notna()][\"Subs\"]\n",
    "            .str.contains(word, case=False)\n",
    "            .any()\n",
    "        ):\n",
    "            val = metadatasource[\n",
    "                metadatasource[\"Subs\"].notna()\n",
    "                & metadatasource[\"Subs\"].str.contains(word, case=False)\n",
    "            ][\"Subs\"].values\n",
    "            for v in val:\n",
    "                # f[v] = \"Substancia\"\n",
    "                f[\"Substancia\"].append(v)\n",
    "    return f\n",
    "metadatasource = pd.read_csv(\"finaldbpt2.csv\", delimiter=\",\")\n",
    "\n",
    "def build_rag_pipeline(products, metadatasource):\n",
    "\n",
    "    llm = Ollama(\n",
    "        model=\"llama3.1:70b\",\n",
    "        base_url=LLM_URL,\n",
    "        temperature=0,\n",
    "        request_timeout=120,\n",
    "    )\n",
    "    print(\"Building index...\")\n",
    "    index = retrieve_index(client, llm, index_name)\n",
    "    print(\"Constructing query engine...\")\n",
    "    filters_qdrant = get_filters_qdrant(\n",
    "        products=products, metadatasource=metadatasource\n",
    "    )\n",
    "    print(\"filtro\", filters_qdrant)\n",
    "\n",
    "    retriever = VectorIndexRetriever(\n",
    "        vector_store_kwargs={\"qdrant_filters\": filters_qdrant},\n",
    "        index=index,\n",
    "        # filters=filters,\n",
    "        similarity_top_k=30,\n",
    "    )\n",
    "\n",
    "    print(retriever.retrieve(\"adalimumab para vacinas é ok?\"))\n",
    "    # configure response synthesizer\n",
    "    # response_synthesizer = get_response_synthesizer()\n",
    "    # reranker = SentenceTransformerRerank(\n",
    "    #    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=15\n",
    "    # )\n",
    "   # reranker = CohereRerank(api_key=cohere_api_key, top_n=15)\n",
    "    # assemble query engine\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,  # response_mode=\"compact\",\n",
    "      #  node_postprocessors=[\n",
    "      #     reranker\n",
    "      #  ],  # ,SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    "    )\n",
    "    return query_engine\n",
    "\n",
    "rag_chain = build_rag_pipeline(products=\"adalimumab\", metadatasource=metadatasource)\n",
    "rag_chain.query(\"adalimumab com vacinas?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragepi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
